import os
from datetime import datetime
from hadesflow.methods.paths import (
    filelist_path,
    log_path,
    tmp_par_path,
    pars_path,
    tmp_log_path,
)


# Create "{label}-{tier}.gen", based on "{label}.keylist" via
# "{label}-{tier}.filelist". Will implicitly trigger creation of all files
# in "{label}-{tier}.filelist".
# Example: "all-char_data[-{detector}[-{campaign}[-{measurement}[-{run}[-{timestamp}]]]]]-{tier}.gen":
rule autogen_output:
    """
    This is the main rule for running the data production,
    it is specified with:
    all-(experiment)-(detector)-(campaign)-(measurement)-(run)-(timestamp)-'tier'.gen
    It will run the complete run script which collects all warnings
    and errors in log files into a final summary file. Also runs the file_db
    generation on new files as well as generating the json file with channels
    and fields in each file.
    """
    input:
        filelist=Path(filelist_path(config)) / "{label}-{tier}.filelist",
    output:
        gen_output="{label}-{tier}.gen",
        summary_log=f"{log_path(config)}/summary-"
        + "{label}-{tier}"
        + f"-{datetime.strftime(datetime.utcnow(), '%Y%m%dT%H%M%SZ')}.log",
        warning_log=f"{log_path(config)}/warning-"
        + "{label}-{tier}"
        + f"-{datetime.strftime(datetime.utcnow(), '%Y%m%dT%H%M%SZ')}.log",
    params:
        log_path=tmp_log_path(config),
        filedb_path=os.path.join(pars_path(config), "filedb"),
        config=lambda wildcards: config,
        basedir=basedir,
        ignore_keys_file=Path(dataflow_configs) / "ignored_cycles.yaml",
    threads: min(workflow.cores, 64)
    script:
        "../src/hadesflow/scripts/flow/complete_run.py"
